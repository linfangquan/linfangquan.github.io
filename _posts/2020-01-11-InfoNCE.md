---
layout: posts
title:  "InfoNCE Revisited"
date:   2020-01-11 23:56:55 +0800
categories: machine-learning
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# 1 What is InfoNCE

InfoNCE modifies the objective function of NCE
$$
J_T(\theta) = \frac{1}{2T}\sum_{t=1}^T{\bigg[\sum_{i=1}^{M}\ln[h(\textbf{w}_{t,d,i};\textbf{c}_t;\theta)] + \sum_{j=1}^{K}\ln[1-h(\textbf{w}_{t,n,j};\textbf{c}_t;\theta)]\bigg]}
$$


to
$$
J_T(\theta) = \frac{1}{2T}\sum_{t=1}^T{\bigg[
\ln{\frac{h(w_{t,0};c_t;\theta)}{\sum_{j=0}^{K}{h(w_{t,j};c_t;\theta)}}}
\bigg]}
$$
where
$$
h(w;c;\theta) \propto \exp(G(w;c;\theta))
$$
and
$$
\begin{equation}
\begin{split}
G(w;c;\theta) &= \ln{p_m(w;c;\theta)} - \ln{p_n(w;c)}\\
&=\ln{p_m(w\|c;\theta) - \ln{p_n(w)}}
\end{split}
\end{equation}
$$
. 

For each $t$  from $1$ to $T$, $w_{t,0}$  is sampled from observed data distribution $p_d(w\|c)$ , while $w_{t,j}$ is sampled from noise distribution $p_n(w)$ when $j \ge 1$ .



Notice that 
$$
p_m(w\|c;\theta) \propto h(w;c;\theta) \cdot p_n(w)
$$




# 2 Why InfoNCE is NCE

InfoNCE is still a Maximum Likelihood Estimation when we consider sample $t$ as a whole which is $(c_t;w_{t,0};...;w_{t,K})$ .


$$
\begin{equation}
\begin{split}

p(l=0\|c_t;w_{t,0};...;w_{t,K}) &= \frac{p(l=0;c_t;w_{t,0};...;w_{t,K})}{\sum_{j=0}^Kp(l=j;c_t;w_{t,0};...;w_{t,K})} \\

&= \frac{p(w_{t,0};...;w_{t,K}\|l=0;c_t)p(l=0)p(c_t)}{\sum_{j=0}^Kp(l=j;c_t;w_{t,0};...;w_{t,K})} \\

&= \frac{p_m(w_{t,0}\|c_t;\theta)\prod_{j=1}^K[p_n(w_{t,j})]p(l=0)p(c_t)}{\sum_{j=0}^K\Big[p_m(w_{t,j}\|c_t;\theta)\prod_{k=0}^K[I(k\ne j)p_n(w_{t,k})]p(l=j)p(c_t)\Big]} \\

&= \frac{p_m(w_{t,0}\|c_t;\theta)\prod_{j=1}^K[p_n(w_{t,j})]}{\sum_{j=0}^K\Big[p_m(w_{t,j}\|c_t;\theta)\prod_{k=0}^K[I(k\ne j)p_n(w_{t,k})]\Big]} \\

&= \frac{h(w_{t,0};c_t;\theta) \cdot p_n(w_{t,0})\prod_{j=1}^K[p_n(w_{t,j})]}{\sum_{j=0}^K\Big[h(w_{t,j};c_t;\theta) \cdot p_n(w_{t,j})\prod_{k=0}^K[I(k\ne j)p_n(w_{t,k})]\Big]} \\

&= \frac{h(w_{t,0};c_t;\theta)\prod_{j=0}^K[p_n(w_{t,j})]}{\sum_{j=0}^K\Big[h(w_{t,j};c_t;\theta))\prod_{k=0}^K[p_n(w_{t,k})]\Big]}\\

&= \frac{h(w_{t,0};c_t;\theta)}{\sum_{j=0}^K\Big[h(w_{t,j};c_t;\theta))\Big]}

\end{split}
\end{equation}
$$




MLE objective function can be written as
$$
\begin{equation}\begin{split}J_T(\theta) &= \sum_{t=1}^{T}\ln\bigg[p(l=0\|c_t;w_{t,0};...;w_{t,K})\bigg] \\&=\sum_{t=1}^{T}\ln\bigg[						\frac{h(w_{t,0};c_t;\theta)}{\sum_{j=0}^K\Big[h(w_{t,j};c_t;\theta))\Big]}						\bigg] \end{split}\end{equation}
$$

